{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUccJYPCIek1",
        "outputId": "4dfd4244-7b3c-45fc-b6b6-acfbefd8d342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ishanikathuria/handwritten-signature-datasets?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 290M/290M [00:04<00:00, 74.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/ishanikathuria/handwritten-signature-datasets/versions/3\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"ishanikathuria/handwritten-signature-datasets\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:57:34.794761Z",
          "iopub.status.busy": "2024-12-12T22:57:34.794155Z",
          "iopub.status.idle": "2024-12-12T22:57:48.498088Z",
          "shell.execute_reply": "2024-12-12T22:57:48.497506Z",
          "shell.execute_reply.started": "2024-12-12T22:57:34.794727Z"
        },
        "id": "xfB-u3Sz2Aff",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:54:45.295136Z",
          "iopub.status.busy": "2024-12-12T22:54:45.294408Z",
          "iopub.status.idle": "2024-12-12T22:54:45.299505Z",
          "shell.execute_reply": "2024-12-12T22:54:45.298619Z",
          "shell.execute_reply.started": "2024-12-12T22:54:45.295101Z"
        },
        "id": "oS8k3l2e_2z6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bengali_directory = os.path.join(path, 'BHSig260-Bengali/BHSig260-Bengali')\n",
        "hindi_directory = os.path.join(path, 'BHSig260-Hindi/BHSig260-Hindi')\n",
        "cedar_directory = os.path.join(path, 'CEDAR/CEDAR')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:52.742932Z",
          "iopub.status.busy": "2024-12-12T22:52:52.742208Z",
          "iopub.status.idle": "2024-12-12T22:52:52.748125Z",
          "shell.execute_reply": "2024-12-12T22:52:52.747210Z",
          "shell.execute_reply.started": "2024-12-12T22:52:52.742899Z"
        },
        "id": "7Y8q3JcoIZnT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
        "    \"\"\"Load and preprocess image with error handling\"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
        "    img = cv2.resize(img, target_size)  # Resize to VGG16 input size\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "\n",
        "    # Thresholding to highlight signature\n",
        "    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Convert to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Convert to array and expand dimensions for batch processing by preprocess_input\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Apply VGG16 preprocessing function\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img[0]  # Remove batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:53.670314Z",
          "iopub.status.busy": "2024-12-12T22:52:53.669619Z",
          "iopub.status.idle": "2024-12-12T22:52:53.681345Z",
          "shell.execute_reply": "2024-12-12T22:52:53.680553Z",
          "shell.execute_reply.started": "2024-12-12T22:52:53.670270Z"
        },
        "id": "Uq7FPWBTIZnU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path):\n",
        "    \"\"\"Load and prepare dataset for training with robust error handling\"\"\"\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise FileNotFoundError(f\"Dataset path not found: {dataset_path}\")\n",
        "\n",
        "    person_dirs = [d for d in os.listdir(dataset_path)\n",
        "                  if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "    if not person_dirs:\n",
        "        raise ValueError(f\"No person directories found in {dataset_path}\")\n",
        "\n",
        "    person_dirs = sorted(person_dirs)\n",
        "\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for person_dir in person_dirs:\n",
        "        full_path = os.path.join(dataset_path, person_dir)\n",
        "\n",
        "        image_files = os.listdir(full_path)\n",
        "        genuine_files = [f for f in image_files if 'G' in f or 'original' in f]\n",
        "        forged_files = [f for f in image_files if 'F' in f or 'forgeries' in f]\n",
        "\n",
        "        if not genuine_files or not forged_files:\n",
        "            print(f\"Warning: Missing genuine or forged files for {person_dir}\")\n",
        "            continue\n",
        "        for i in range(len(genuine_files)):\n",
        "            for j in range(i + 1, len(genuine_files)):\n",
        "                try:\n",
        "                    anchor = load_and_preprocess_image(os.path.join(full_path, genuine_files[i]))\n",
        "                    positive = load_and_preprocess_image(os.path.join(full_path, genuine_files[j]))\n",
        "                    pairs.append([anchor, positive])\n",
        "                    labels.append(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing positive pair: {e}\")\n",
        "                    continue\n",
        "\n",
        "        for i in range(min(len(genuine_files), len(forged_files))):\n",
        "            try:\n",
        "                anchor = load_and_preprocess_image(os.path.join(full_path, genuine_files[i]))\n",
        "                negative = load_and_preprocess_image(os.path.join(full_path, forged_files[i]))\n",
        "                pairs.append([anchor, negative])\n",
        "                labels.append(0)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing negative pair: {e}\")\n",
        "                continue\n",
        "\n",
        "    if not pairs:\n",
        "        raise ValueError(f\"No valid pairs could be created from {dataset_path}\")\n",
        "\n",
        "    pairs = np.array(pairs)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    train_pairs, test_pairs, train_labels, test_labels = train_test_split(\n",
        "        pairs, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    train_pairs, val_pairs, train_labels, val_labels = train_test_split(\n",
        "        train_pairs, train_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    return (train_pairs, train_labels), (val_pairs, val_labels), (test_pairs, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ8ov73i4xLB"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:54.584800Z",
          "iopub.status.busy": "2024-12-12T22:52:54.584536Z",
          "iopub.status.idle": "2024-12-12T22:52:54.590111Z",
          "shell.execute_reply": "2024-12-12T22:52:54.589193Z",
          "shell.execute_reply.started": "2024-12-12T22:52:54.584776Z"
        },
        "id": "d4pnkYEHIZnU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_data_generators(train_pairs, train_labels, val_pairs, val_labels, test_pairs, test_labels, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Create data generators for training and validation.\"\"\"\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_layer_1': train_pairs[:, 0],\n",
        "            'input_layer_2': train_pairs[:, 1]\n",
        "        },\n",
        "        train_labels\n",
        "    )).shuffle(128).batch(batch_size).prefetch(AUTO)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_layer_1': val_pairs[:, 0],\n",
        "            'input_layer_2': val_pairs[:, 1]\n",
        "        },\n",
        "        val_labels\n",
        "    )).batch(batch_size).prefetch(AUTO)\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_layer_1': test_pairs[:, 0],\n",
        "            'input_layer_2': test_pairs[:, 1]\n",
        "        },\n",
        "        test_labels\n",
        "    )).batch(batch_size).prefetch(AUTO)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:55.860634Z",
          "iopub.status.busy": "2024-12-12T22:52:55.860289Z",
          "iopub.status.idle": "2024-12-12T22:52:55.865475Z",
          "shell.execute_reply": "2024-12-12T22:52:55.864594Z",
          "shell.execute_reply.started": "2024-12-12T22:52:55.860605Z"
        },
        "id": "cNebnPKZIZnU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vectors):\n",
        "    x, y = vectors\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:56.532989Z",
          "iopub.status.busy": "2024-12-12T22:52:56.532698Z",
          "iopub.status.idle": "2024-12-12T22:52:56.537533Z",
          "shell.execute_reply": "2024-12-12T22:52:56.536756Z",
          "shell.execute_reply.started": "2024-12-12T22:52:56.532962Z"
        },
        "id": "AGPyvbv6IZnU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean((1 - y_true) * square_pred + y_true * margin_square)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:52:57.308693Z",
          "iopub.status.busy": "2024-12-12T22:52:57.307886Z",
          "iopub.status.idle": "2024-12-12T22:53:01.384618Z",
          "shell.execute_reply": "2024-12-12T22:53:01.383925Z",
          "shell.execute_reply.started": "2024-12-12T22:52:57.308660Z"
        },
        "id": "Z8W_lSbL2Afg",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988b58a0-1722-4784-9385-58c096421362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "embedding = Dense(128)(x)\n",
        "\n",
        "embedding_model = Model(base_model.input, embedding)\n",
        "\n",
        "input_1 = Input(shape=(224, 224, 3))\n",
        "input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "embedding_1 = embedding_model(input_1)\n",
        "embedding_2 = embedding_model(input_2)\n",
        "\n",
        "distance = Lambda(euclidean_distance)([embedding_1, embedding_2])\n",
        "\n",
        "predictions = Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "siamese_model = Model(inputs=[input_1, input_2], outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:53:02.636740Z",
          "iopub.status.busy": "2024-12-12T22:53:02.636446Z",
          "iopub.status.idle": "2024-12-12T22:53:02.650067Z",
          "shell.execute_reply": "2024-12-12T22:53:02.649217Z",
          "shell.execute_reply.started": "2024-12-12T22:53:02.636716Z"
        },
        "id": "8lTVFcSe2Afg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "siamese_model.compile(optimizer='adam', metrics=['accuracy'], loss = contrastive_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0j5uHXTtsi3"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IElrhC3IZnW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cedar_train, cedar_val, cedar_test = load_dataset(cedar_directory)\n",
        "\n",
        "cedar_train_gen, cedar_val_gen, cedar_test_gen = create_data_generators(\n",
        "    cedar_train[0], cedar_train[1],\n",
        "    cedar_val[0], cedar_val[1],\n",
        "    cedar_test[0], cedar_test[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T21:30:41.960270Z",
          "iopub.status.busy": "2024-12-12T21:30:41.959938Z",
          "iopub.status.idle": "2024-12-12T21:30:42.026878Z",
          "shell.execute_reply": "2024-12-12T21:30:42.025652Z",
          "shell.execute_reply.started": "2024-12-12T21:30:41.960242Z"
        },
        "id": "f2ZEj2W3tBlc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history3 = siamese_model.fit(\n",
        "    cedar_train_gen,\n",
        "    validation_data=cedar_val_gen,\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLUcFtbttCt5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history3.history['accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history3.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRNv5OhOeQ4m"
      },
      "outputs": [],
      "source": [
        "loss3, accuracy3 = siamese_model.evaluate(cedar_test_gen)\n",
        "print(f\"Test Loss: {loss3}\")\n",
        "print(f\"Test Accuracy: {accuracy3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T22:54:49.592102Z",
          "iopub.status.busy": "2024-12-12T22:54:49.591564Z",
          "iopub.status.idle": "2024-12-12T22:54:49.599836Z",
          "shell.execute_reply": "2024-12-12T22:54:49.598810Z",
          "shell.execute_reply.started": "2024-12-12T22:54:49.592067Z"
        },
        "id": "EGZLKDrOIZnV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bengali_train, bengali_val, bengali_test = load_dataset(bengali_directory)\n",
        "\n",
        "bengali_train_gen, bengali_val_gen, bengali_test_gen = create_data_generators(\n",
        "    bengali_train[0], bengali_train[1],\n",
        "    bengali_val[0], bengali_val[1],\n",
        "    bengali_test[0], bengali_test[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQDv5BbrdePy"
      },
      "outputs": [],
      "source": [
        "loss1, accuracy1 = siamese_model.evaluate(bengali_test_gen)\n",
        "print(f\"Test Loss: {loss1}\")\n",
        "print(f\"Test Accuracy: {accuracy1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_J3kVvuIZnW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "hindi_train, hindi_val, hindi_test = load_dataset(hindi_directory)\n",
        "\n",
        "hindi_train_gen, hindi_val_gen, hindi_test_gen = create_data_generators(\n",
        "    hindi_train[0], hindi_train[1],\n",
        "    hindi_val[0], hindi_val[1],\n",
        "    hindi_test[0], hindi_test[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xONMqogd_Hp"
      },
      "outputs": [],
      "source": [
        "loss2, accuracy2 = siamese_model.evaluate(hindi_test_gen)\n",
        "print(f\"Test Loss: {loss2}\")\n",
        "print(f\"Test Accuracy: {accuracy2}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 1384931,
          "sourceId": 2359135,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}